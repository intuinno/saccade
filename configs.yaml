defaults:
  behavior: 'imagineAC' # vanilla_AC or imagine_AC
  envs: 8
  parallel: True
  num_loc_per_side: 4
  intrinsic_reward: 'none'
  batch_size: 128
  eval_batch_size: 512
  exp_name: 'default'
  logdir: 'logs'
  load_wm: 'models/sac-010-rev2-random-1M.pt'
  traindir: null
  offline_traindir: ''
  seed: 0
  deterministic_run: False
  steps: 1e9
  eval_every: 50000
  log_every: 10000
  reset_every: 0
  device: 'cuda:0'
  compile: True
  precision: 32
  debug: False

  # Environment
  task: 'saccade_MMNIST'
  size: [64, 64]
  action_repeat: 1
  time_limit: 1000
  grayscale: False
  prefill: 2500
  reward_EMA: True

  # Model
  dyn_hidden: 512
  dyn_deter: 512
  dyn_stoch: 32
  dyn_discrete: 32
  dyn_rec_depth: 1
  dyn_mean_act: 'none'
  dyn_std_act: 'sigmoid2'
  dyn_min_std: 0.1
  grad_heads: ['decoder', 'reward', 'cont']
  units: 512
  act: 'SiLU'
  norm: True
  encoder:
    {mlp_keys: 'central|peripheral', cnn_keys: '$^', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, symlog_inputs: True}
  decoder:
    {mlp_keys: 'central|peripheral', cnn_keys: '$^', act: 'SiLU', norm: True, cnn_depth: 32, kernel_size: 4, minres: 4, mlp_layers: 5, mlp_units: 1024, cnn_sigmoid: False, image_dist: mse, vector_dist: symlog_mse, outscale: 1.0}
  actor:
    {layers: 3, dist: 'onehot', entropy: 3e-4, unimix_ratio: 0.01, std: 'none', min_std: 0.1, max_std: 1.0, temp: 0.1, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 1.0}
  critic:
    {layers: 2, dist: 'symlog_disc', slow_target: True, slow_target_update: 1, slow_target_fraction: 0.02, lr: 3e-5, eps: 1e-5, grad_clip: 100.0, outscale: 0.0}
  reward_head:
    {layers: 2, dist: 'symlog_disc', loss_scale: 1.0, outscale: 0.0}
  cont_head:
    {layers: 2, loss_scale: 1.0, outscale: 1.0}
  dyn_scale: 0.5
  rep_scale: 0.1
  kl_free: 1.0
  weight_decay: 0.0
  unimix_ratio: 0.01
  initial: 'learned'

  # Training
  total_eval_batch_size: 10240
  batch_length: 64
  train_ratio: 512
  pretrain: 100
  model_lr: 1e-4
  opt_eps: 1e-8
  grad_clip: 1000
  dataset_size: 10000000
  opt: 'adam'

  # Behavior.
  discount_gamma: 0.997
  discount_lambda: 0.95
  imag_horizon: 15
  imag_gradient: 'dynamics'
  imag_gradient_mix: 0.0
  eval_state_mean: False

  # Exploration
  expl_behavior: 'greedy'
  expl_until: 0
  expl_extr_scale: 0.0
  expl_intr_scale: 1.0
  disag_target: 'stoch'
  disag_log: True
  disag_models: 10
  disag_offset: 1
  disag_layers: 4
  disag_units: 400
  disag_action_cond: False


sac-012-rev1:
  exp_name: 'sac-012-rev1'
  intrinsic_reward: 'prediction_error'
  behavior: 'vanilla_AC'

sac-012-rev2:
  exp_name: 'sac-012-rev2'
  intrinsic_reward: 'disagreement'
  behavior: 'imagine'

debug:
  debug: True
  pretrain: 1
  prefill: 1000
  batch_size: 4
  eval_batch_size: 8
  batch_length: 20
  device: 'cpu'
  parallel: False
  exp_name: 'debug-'
  compile: False
  total_eval_batch_size: 128
  eval_every: 128
  train_ratio: 64